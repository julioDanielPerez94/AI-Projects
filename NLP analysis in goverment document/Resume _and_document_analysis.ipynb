{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f90982e",
   "metadata": {},
   "source": [
    "# Edosoft - Resumen y clasificaciÃ³n de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c413f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "import string\n",
    "import math\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.es import Spanish\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "import PyPDF2\n",
    "from tika import parser\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f86a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"prueba_es.txt\", \"r\", encoding='utf8')\n",
    "lines = file.readlines()\n",
    "\n",
    "datos = {'Documentos':lines}\n",
    "df = pd.DataFrame(data = datos)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1599d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Documentos'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d05c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### Primero tokenizamos los 4 primeros documentos\n",
    "# def tokenize_words(text):\n",
    "#     sin_puntuacion = text.translate(str.maketrans('', '', string.punctuation))\n",
    "#     tokenized_text=word_tokenize(sin_puntuacion)\n",
    "#     print(tokenized_text)\n",
    "#     print(type(tokenized_text))\n",
    "    \n",
    "#     return tokenized_text\n",
    "\n",
    "# # df_tokenized = df.copy()\n",
    "# df['Documentos'] = df['Documentos'].apply(lambda x: tokenize_words(x))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87665b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword_es = nltk.corpus.stopwords.words('spanish')\n",
    "# #from nltk and spacy library\n",
    "# total_spanish_stopwords = list(stopwords.words(\"spanish\")) + stopword_es\n",
    "\n",
    "# def remove_stopwords(text):\n",
    "#     text = [word for word in text if word not in stopword_es]\n",
    "#     return text\n",
    "\n",
    "# df['Sin_stopwords'] = df['Documentos'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbe72ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['Sin_stopwords'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f4dfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def spanish_stemmer(x):\n",
    "#     spanish_stemmer_ = SnowballStemmer(language = 'spanish')\n",
    "#     for i in range(len(x)):\n",
    "# #         print(i)\n",
    "#         for l in range(len(x[i])):\n",
    "# #             print(l)\n",
    "#             x[i][l] = spanish_stemmer_.stem(x[i][l])\n",
    "# #             print(x[i])\n",
    "#     return x\n",
    "    \n",
    "# df['words_stemmed'] = spanish_stemmer(df['Sin_stopwords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c194579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['words_stemmed'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881894c3",
   "metadata": {},
   "source": [
    "## Empezamos a hacer el resumen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df9bbac",
   "metadata": {},
   "source": [
    "Trying with the following guide: [Click on here](https://towardsdatascience.com/text-summarization-using-tf-idf-e64a0644ace3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3b394",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Documentos'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e6524",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tokenize(df['Documentos'][0]) # NLTK function\n",
    "total_documents = len(sentences)\n",
    "\n",
    "stopword_es = nltk.corpus.stopwords.words('spanish')\n",
    "#from nltk and spacy library\n",
    "total_spanish_stopwords = list(stopwords.words(\"spanish\")) + stopword_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47959214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d5c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_frequency_matrix(sentences):\n",
    "    frequency_matrix = {}\n",
    "    \n",
    "    stopword_es = nltk.corpus.stopwords.words('spanish')\n",
    "    #from nltk and spacy library\n",
    "    total_spanish_stopwords = list(stopwords.words(\"spanish\")) + stopword_es\n",
    "    stopWords = total_spanish_stopwords\n",
    "    ps = SnowballStemmer(language = 'spanish')\n",
    "\n",
    "    for sent in sentences:\n",
    "        freq_table = {}\n",
    "        words = word_tokenize(sent)\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            word = ps.stem(word)\n",
    "            if word in stopWords:\n",
    "                continue\n",
    "\n",
    "            if word in freq_table:\n",
    "                freq_table[word] += 1\n",
    "            else:\n",
    "                freq_table[word] = 1\n",
    "\n",
    "        frequency_matrix[sent[:15]] = freq_table\n",
    "\n",
    "    return frequency_matrix\n",
    "\n",
    "def _create_tf_matrix(freq_matrix):\n",
    "    tf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        tf_table = {}\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, count in f_table.items():\n",
    "            tf_table[word] = count / count_words_in_sentence\n",
    "\n",
    "        tf_matrix[sent] = tf_table\n",
    "\n",
    "    return tf_matrix\n",
    "\n",
    "def _create_documents_per_words(freq_matrix):\n",
    "    word_per_doc_table = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        for word, count in f_table.items():\n",
    "            if word in word_per_doc_table:\n",
    "                word_per_doc_table[word] += 1\n",
    "            else:\n",
    "                word_per_doc_table[word] = 1\n",
    "\n",
    "    return word_per_doc_table\n",
    "\n",
    "\n",
    "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
    "    idf_matrix = {}\n",
    "\n",
    "    for sent, f_table in freq_matrix.items():\n",
    "        idf_table = {}\n",
    "\n",
    "        for word in f_table.keys():\n",
    "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
    "\n",
    "        idf_matrix[sent] = idf_table\n",
    "\n",
    "    return idf_matrix\n",
    "\n",
    "\n",
    "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
    "    tf_idf_matrix = {}\n",
    "\n",
    "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
    "\n",
    "        tf_idf_table = {}\n",
    "\n",
    "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
    "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
    "            tf_idf_table[word1] = float(value1 * value2)\n",
    "\n",
    "        tf_idf_matrix[sent1] = tf_idf_table\n",
    "\n",
    "    return tf_idf_matrix\n",
    "\n",
    "def _score_sentences(tf_idf_matrix) -> dict:\n",
    "    \"\"\"\n",
    "    score a sentence by its word's TF\n",
    "    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
    "    :rtype: dict\n",
    "    \"\"\"\n",
    "\n",
    "    sentenceValue = {}\n",
    "\n",
    "    for sent, f_table in tf_idf_matrix.items():\n",
    "        total_score_per_sentence = 0\n",
    "\n",
    "        count_words_in_sentence = len(f_table)\n",
    "        for word, score in f_table.items():\n",
    "            total_score_per_sentence += score\n",
    "\n",
    "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
    "\n",
    "    return sentenceValue\n",
    "\n",
    "\n",
    "def _find_average_score(sentenceValue) -> int:\n",
    "    \"\"\"\n",
    "    Find the average score from the sentence value dictionary\n",
    "    :rtype: int\n",
    "    \"\"\"\n",
    "    sumValues = 0\n",
    "    for entry in sentenceValue:\n",
    "        sumValues += sentenceValue[entry]\n",
    "\n",
    "    # Average value of a sentence from original summary_text\n",
    "    average = (sumValues / len(sentenceValue))\n",
    "\n",
    "    return average\n",
    "\n",
    "\n",
    "def _generate_summary(sentences, sentenceValue, threshold):\n",
    "    sentence_count = 0\n",
    "    summary = ''\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
    "            summary += \" \" + sentence\n",
    "            sentence_count += 1\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7270062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_matrix= _create_frequency_matrix(sentences)\n",
    "\n",
    "tf_matrix = _create_tf_matrix(freq_matrix)\n",
    "\n",
    "count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
    "\n",
    "idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "\n",
    "tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "\n",
    "sentence_scores = _score_sentences(tf_idf_matrix)\n",
    "\n",
    "threshold = _find_average_score(sentence_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ee5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = _generate_summary(sentences, sentence_scores, 1* threshold)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773c7c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d606824c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df['Documentos'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4e773c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf0f700",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_2 = open(\"boe.txt\", \"r\", encoding='utf8')\n",
    "lines_2 = file_2.readlines()\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=[lines,lines_2],columns=['Documento'])\n",
    "df['Documento'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db4108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Documento'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be526116",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b4b67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('boe.txt', encoding='Latin1') as f:\n",
    "    contents = f.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455ee7f",
   "metadata": {},
   "source": [
    "##### https://www.simplifiedpython.net/pdf-to-text-python-extract-text-from-pdf-documents-using-pypdf2-module/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9687da",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfFileObject = open(r\"prueba_es.pdf\", 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObject)\n",
    "print(\" No. Of Pages :\", pdfReader.numPages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fa631",
   "metadata": {},
   "outputs": [],
   "source": [
    "pageObject = pdfReader.getPage(0)\n",
    " \n",
    "print(pageObject.extractText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fb48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = pdfReader.getPage(0).extractText() + pdfReader.getPage(1).extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf26a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto.replace('\\n',' ').lstrip().rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db75a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "texto = \" \".join(texto.replace('\\n',' ').split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba91736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34837707/how-to-extract-text-from-a-pdf-file\n",
    "\n",
    "from tika import parser\n",
    "\n",
    "raw = parser.from_file('prueba_es.pdf')\n",
    "print(raw['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edcd4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\" \".join(raw['content'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23806366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tutorialspoint.com/working-with-pdf-files-in-python\n",
    "\n",
    "# with open('BOE-A-2010-4132.pdf', 'rb') as f:\n",
    "#     pdf = PyPDF2.PdfFileReader(f)\n",
    "#     info = pdf.getDocumentInfo()\n",
    "#     number_of_pages = pdf.getNumPages()\n",
    "\n",
    "# print(\"Author: \\t\", info.author)\n",
    "# print()\n",
    "# print(\"Creator: \\t\", info.creator)\n",
    "# print()\n",
    "# print(\"Producer: \\t\",info.producer)\n",
    "# print()\n",
    "# print(\"Subject: \\t\", info.subject)\n",
    "# print()\n",
    "# print(\"title: \\t\",info.title)\n",
    "# print()\n",
    "# print(\"Number of Pages in pdf: \\t\",number_of_pages)\n",
    "\n",
    "# prueba = ''\n",
    "# for i in range(pdf.getNumPages()):\n",
    "#     prueba = prueba + pdf.getPage(i).extractText()\n",
    "#     print(len(prueba))\n",
    "    \n",
    "# pdf.getPage(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9256ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf.getPage(2).extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd1e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('BOE-A-2010-4132.pdf', 'rb') as f:\n",
    "    pdf = PyPDF2.PdfFileReader(f)\n",
    "    info = pdf.getDocumentInfo()\n",
    "    number_of_pages = pdf.getNumPages()\n",
    "print(\"Author: \\t\", info.author)\n",
    "print()\n",
    "print(\"Creator: \\t\", info.creator)\n",
    "print()\n",
    "print(\"Producer: \\t\",info.producer)\n",
    "print()\n",
    "print(\"Subject: \\t\", info.subject)\n",
    "print()\n",
    "print(\"title: \\t\",info.title)\n",
    "print()\n",
    "print(\"Number of Pages in pdf: \\t\",number_of_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6823bc12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdfFileObject = open('BOE-A-2010-4132.pdf', 'rb')\n",
    "# pdfReader = PyPDF2.PdfFileReader(pdfFileObject)\n",
    "# prueba = ''\n",
    "# for i in range(pdfReader.getNumPages()):\n",
    "#     prueba = prueba + pdfReader.getPage(i).extractText()\n",
    "#     print(len(prueba))\n",
    "# prueba = prueba.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19519305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdfReader.getPage(1).extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b1c7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bf45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfReader.getDocumentInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ab465d",
   "metadata": {},
   "outputs": [],
   "source": [
    "documento = 'BOE-A-2010-4132.pdf'\n",
    "\n",
    "with open(documento, 'rb') as f:\n",
    "    pdf = PyPDF2.PdfFileReader(f)\n",
    "    info = pdf.getDocumentInfo()\n",
    "    number_of_pages = pdf.getNumPages()\n",
    "\n",
    "# text = ''\n",
    "# for i in range(pdf.getNumPages()):\n",
    "#     text = text + pdf.getPage(i).extractText()\n",
    "#     print(len(prueba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b1f571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incluir el documento en PDF\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "def resume_pdfs(documento, alpha):\n",
    "    \n",
    "    #### First parser of PDFs files\n",
    "    # ==============================================================================\n",
    "    \n",
    "    pdfFileObject = open(documento, 'rb')\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObject)\n",
    "#     text = ''\n",
    "#     for i in range(pdfReader.getNumPages()):\n",
    "#         text = text + pdfReader.getPage(i).extractText()\n",
    "#     text = text.replace('\\n',' ')\n",
    "#     print(text)\n",
    "#         print(len(prueba))\n",
    "      \n",
    "    \n",
    "    #### Second parser of PDFs files\n",
    "    # ==============================================================================\n",
    "    raw = parser.from_file(documento)\n",
    "#     print(raw['content'])\n",
    "\n",
    "    text = \" \".join(raw['content'].split())\n",
    "\n",
    "    # -----------------------------------\n",
    "    \n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    total_documents = len(sentences)\n",
    "    #print(sentences)\n",
    "\n",
    "    # 2 Create the Frequency matrix of the words in each sentence.\n",
    "    freq_matrix = _create_frequency_matrix(sentences)\n",
    "    #print(freq_matrix)\n",
    "\n",
    "    '''\n",
    "    Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document.\n",
    "    '''\n",
    "    # 3 Calculate TermFrequency and generate a matrix\n",
    "    tf_matrix = _create_tf_matrix(freq_matrix)\n",
    "    #print(tf_matrix)\n",
    "\n",
    "    # 4 creating table for documents per words\n",
    "    count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
    "    #print(count_doc_per_words)\n",
    "\n",
    "    '''\n",
    "    Inverse document frequency (IDF) is how unique or rare a word is.\n",
    "    '''\n",
    "    # 5 Calculate IDF and generate a matrix\n",
    "    idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
    "    #print(idf_matrix)\n",
    "\n",
    "    # 6 Calculate TF-IDF and generate a matrix\n",
    "    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
    "    #print(tf_idf_matrix)\n",
    "\n",
    "    # 7 Important Algorithm: score the sentences\n",
    "    sentence_scores = _score_sentences(tf_idf_matrix)\n",
    "    #print(sentence_scores)\n",
    "\n",
    "    # 8 Find the threshold\n",
    "    threshold = _find_average_score(sentence_scores)\n",
    "    #print(threshold)\n",
    "\n",
    "    # 9 Important Algorithm: Generate the summary (alpha between 0.5 - 1.5)\n",
    "    summary = _generate_summary(sentences, sentence_scores, alpha * threshold)\n",
    "    \n",
    "    \n",
    "    print(\"TÃ­tulo: \\t\",pdfReader.getDocumentInfo().title)\n",
    "    print('--------------------------------------------------------------------')\n",
    "    print(\"NÃºmero de pÃ¡ginas: {} \\nLongitud del documento original: {}\\nLongitud del resumen: {}\".format(pdfReader.getNumPages(), len(text), len(summary)))\n",
    "    print('--------------------------------------------------------------------')\n",
    "    print(summary)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "011c7f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TÃ­tulo: \t DisposiciÃ³n 4132 del BOE nÃºm. 62 de 2010\n",
      "--------------------------------------------------------------------\n",
      "NÃºmero de pÃ¡ginas: 10 \n",
      "Longitud del documento original: 33619\n",
      "Longitud del resumen: 8502\n",
      "--------------------------------------------------------------------\n",
      " DisposiciÃ³n 4132 del BOE nÃºm. 62 de 2010 BOLETÃN OFICIAL DEL ESTADO NÃºm. 62 Viernes 12 de marzo de 2010 Sec. I. PÃ¡g. cv e: B O E -A -2 01 0- 41 32 BOLETÃN OFICIAL DEL ESTADO NÃºm. 62 Viernes 12 de marzo de 2010 Sec. I. PÃ¡g. Objeto. 1. Los requisitos mÃ­nimos se referirÃ¡n a la titulaciÃ³n acadÃ©mica del profesorado, la relaciÃ³n numÃ©rica alumno-profesor, las instalaciones docentes y deportivas y el nÃºmero de puestos escolares. 2. ArtÃ­culo 2. DenominaciÃ³n de los centros docentes. ArtÃ­culo 3. Requisitos de instalaciones comunes a todos los centros. 1. 2. b) Reunir las condiciones de seguridad estructural, de seguridad en caso de incendio, de seguridad de utilizaciÃ³n, de salubridad, de protecciÃ³n frente al ruido y de ahorro de energÃ­a que seÃ±ala la legislaciÃ³n vigente. Asimismo, deberÃ¡n cumplir los requisitos de protecciÃ³n laboral establecidos en la legislaciÃ³n vigente. c) Tener, en los espacios en los que se desarrolle la prÃ¡ctica docente ventilaciÃ³n e iluminaciÃ³n natural y directa desde el exterior. cv e: B O E -A -2 01 0- 41 32 BOLETÃN OFICIAL DEL ESTADO NÃºm. 62 Viernes 12 de marzo de 2010 Sec. I. PÃ¡g. Espacios destinados a la administraciÃ³n. Sala de profesores adecuada al nÃºmero de profesores. Espacios apropiados para las reuniones de las asociaciones de alumnos y de madres y padres de alumnos, en el caso de centros sostenidos con fondos pÃºblicos. Espacios necesarios para impartir los apoyos al alumnado con necesidades especÃ­ficas de apoyo educativo. 3. En ningÃºn caso serÃ¡ inferior 900 metros cuadrados. Biblioteca, con una superficie, como mÃ­nimo, de 45 metros cuadrados en los centros que impartan la educaciÃ³n primaria, y 75 metros cuadrados en los centros que impartan la educaciÃ³n secundaria obligatoria o el bachillerato. Un gimnasio con una superficie adecuada al nÃºmero de puestos escolares. 4. Los requisitos de instalaciones podrÃ¡n flexibilizarse de acuerdo con lo establecido en el artÃ­culo 20 y la disposiciÃ³n adicional tercera de este real decreto. ArtÃ­culo 4. Puestos escolares. 1. 2. TÃTULO II De los centros de educaciÃ³n infantil ArtÃ­culo 5. Condiciones generales. 1. Los centros podrÃ¡n ofrecer el primer ciclo de esta etapa educativa, el segundo, o ambos. 2. cv e: B O E -A -2 01 0- 41 32 BOLETÃN OFICIAL DEL ESTADO NÃºm. 62 Viernes 12 de marzo de 2010 Sec. I. PÃ¡g. 24834 ArtÃ­culo 6. Instalaciones y condiciones materiales de los centros que ofrecen el segundo ciclo de la educaciÃ³n infantil. 1. 2. b) Una sala polivalente de 30 metros cuadrados. ArtÃ­culo 7. RelaciÃ³n alumnos por unidad. ArtÃ­culo 8. Requisitos de titulaciÃ³n de los profesionales que atienden la educaciÃ³n infantil. 1. 2. 3. El segundo ciclo de educaciÃ³n infantil correrÃ¡ a cargo de profesionales que posean el tÃ­tulo de Grado en EducaciÃ³n infantil, o el tÃ­tulo de Maestro con la especialidad de educaciÃ³n infantil. Cuando las enseÃ±anzas impartidas lo requieran, el grupo podrÃ¡ ser atendido por maestros de otras especialidades. 4. 5. 6. Los recursos humanos a que se refiere el pÃ¡rrafo anterior deberÃ¡n disponer de la titulaciÃ³n o cualificaciÃ³n adecuada. cv e: B O E -A -2 01 0- 41 32 BOLETÃN OFICIAL DEL ESTADO NÃºm. 62 Viernes 12 de marzo de 2010 Sec. I. PÃ¡g. 24835 TÃTULO III De los centros de educaciÃ³n primaria ArtÃ­culo 9. Condiciones generales. ArtÃ­culo 10. Instalaciones y condiciones materiales de los centros. b) Un espacio por cada seis unidades para desdoblamiento de grupos y otro para actividades de apoyo y refuerzo pedagÃ³gico. c) Una sala polivalente, con una superficie adecuada al nÃºmero de puestos escolares autorizados, que podrÃ¡ compartimentarse con mamparas mÃ³viles. ArtÃ­culo 11. RelaciÃ³n de alumnos por unidad. ArtÃ­culo 12. Requisitos de titulaciÃ³n acadÃ©mica del profesorado que imparte educaciÃ³n primaria. 1. 2. Los recursos humanos a que se refiere el pÃ¡rrafo anterior deberÃ¡n disponer de la titulaciÃ³n o cualificaciÃ³n adecuada. TÃTULO IV De los centros de educaciÃ³n secundaria ArtÃ­culo 13. Condiciones generales. 1. 2. 3. 62 Viernes 12 de marzo de 2010 Sec. I. PÃ¡g. 4. 5. ArtÃ­culo 14. Instalaciones y condiciones materiales de los centros que imparten educaciÃ³n secundaria obligatoria. c) Al menos un laboratorio de Ciencias Experimentales por cada 12 unidades o fracciÃ³n. d) Un espacio por cada ocho unidades para desdoblamiento de grupos y otro para actividades de apoyo y refuerzo pedagÃ³gico. ArtÃ­culo 15. Instalaciones y condiciones materiales de los centros que imparten bachillerato. 1. 2. Un espacio por cada cuatro unidades para desdoblamiento de grupos y otro para actividades de apoyo y refuerzo pedagÃ³gico. 3. Un aula de mÃºsica cuando se imparta la vÃ­a de artes escÃ©nicas, mÃºsica y danza. b) Para la modalidad de Ciencias y TecnologÃ­a: Tres laboratorios diferenciados de FÃ­sica, QuÃ­mica y Ciencias. Un aula de dibujo. Un aula de TecnologÃ­a. ArtÃ­culo 16. RelaciÃ³n de alumnos por unidad. cv e: B O E -A -2 01 0- 41 32 BOLETÃN OFICIAL DEL ESTADO NÃºm. 62 Viernes 12 de marzo de 2010 Sec. I. PÃ¡g. 24837 ArtÃ­culo 17. Profesionales que atienden la educaciÃ³n secundaria obligatoria y el bachillerato. 1. 2. ArtÃ­culo 18. NÃºmero mÃ­nimo de profesores de los centros que ofrecen la educaciÃ³n secundaria obligatoria y el bachillerato. 1. 2. Este personal deberÃ¡ estar en posesiÃ³n de la titulaciÃ³n o cualificaciÃ³n adecuada. ArtÃ­culo 19. Requisitos de centros que ofrecen los Programas de CualificaciÃ³n Profesional Inicial. 1. 2. ArtÃ­culo 20. FlexibilizaciÃ³n de los requisitos de instalaciones para los centros docentes que impartan distintas enseÃ±anzas en el mismo edificio o recinto escolar. 1. 2. b) El gimnasio. c) El patio de recreo. d) Los despachos de direcciÃ³n, los espacios destinados a la administraciÃ³n y la sala de profesores. cv e: B O E -A -2 01 0- 41 32 BOLETÃN OFICIAL DEL ESTADO NÃºm. 62 Viernes 12 de marzo de 2010 Sec. I. PÃ¡g. 24838 3. Asimismo cubre la exigencia del aula de educaciÃ³n plÃ¡stica y visual para la educaciÃ³n secundaria obligatoria. c) El aula de mÃºsica para el bachillerato de artes en la vÃ­a de artes escÃ©nicas, mÃºsica y danza, cubre la exigencia del aula de mÃºsica para la educaciÃ³n secundaria obligatoria. d) Los laboratorios para el bachillerato de la modalidad de ciencias y tecnologÃ­a cubren la exigencia del laboratorio de ciencias experimentales para la educaciÃ³n secundaria obligatoria. Del mismo modo, el aula de tecnologÃ­a para el bachillerato cubre la exigencia del aula taller de tecnologÃ­as para la educaciÃ³n secundaria obligatoria. 4. DisposiciÃ³n adicional primera. Centros que ofrecen la educaciÃ³n de personas adultas. 1. 2. Los requisitos de instalaciones se adecuarÃ¡n a la organizaciÃ³n especÃ­fica de las enseÃ±anzas de adultos. DisposiciÃ³n adicional segunda. Centros de educaciÃ³n especial. DisposiciÃ³n adicional tercera. Centros que atiendan a poblaciones de especiales caracterÃ­sticas sociodemogrÃ¡ficas. 1. 2. 3. cv e: B O E -A -2 01 0- 41 32 BOLETÃN OFICIAL DEL ESTADO NÃºm. 62 Viernes 12 de marzo de 2010 Sec. I. PÃ¡g. 24839 DisposiciÃ³n adicional cuarta. Centros docentes reconocidos por acuerdos internacionales. Los requisitos de los centros docentes cuyo carÃ¡cter especÃ­fico estÃ© reconocido por acuerdos internacionales de carÃ¡cter bilateral podrÃ¡n ser adaptados por el Ministerio de EducaciÃ³n. DisposiciÃ³n adicional quinta. Centros acogidos al rÃ©gimen de conciertos. DisposiciÃ³n adicional sexta. InscripciÃ³n en Registro. DisposiciÃ³n adicional sÃ©ptima. Centros sometidos al Derecho comÃºn. DisposiciÃ³n adicional octava. Profesionales habilitados. 1. 2. DisposiciÃ³n adicional novena. Referencias genÃ©ricas. Todas las referencias para las que en este real decreto se utiliza la forma de masculino genÃ©rico, deben entenderse aplicables, indistintamente, a mujeres y a hombres. DisposiciÃ³n transitoria primera. Solicitudes de autorizaciÃ³n de nuevos centros. 1. 2. 3. 62 Viernes 12 de marzo de 2010 Sec. I. PÃ¡g. 24840 mismo, se regirÃ¡n, en lo relacionado a las instalaciones, a la normativa en vigor en el momento de realizar dicha aprobaciÃ³n. DisposiciÃ³n transitoria segunda. AcreditaciÃ³n de la habilitaciÃ³n para la docencia. DisposiciÃ³n transitoria tercera. DisposiciÃ³n derogatoria Ãºnica. DerogaciÃ³n normativa. 1. 2. Quedan derogadas cuantas disposiciones de igual o inferior rango se opongan a lo dispuesto en este real decreto. DisposiciÃ³n final primera. TÃ­tulo competencial. DisposiciÃ³n final segunda. Desarrollo normativo. DisposiciÃ³n final tercera. Entrada en vigor. El presente real decreto entrarÃ¡ en vigor el dÃ­a siguiente al de su publicaciÃ³n en el Â«BoletÃ­n Oficial del EstadoÂ». Dado en Madrid, el 12 de febrero de 2010.\n"
     ]
    }
   ],
   "source": [
    "# resume_pdfs('prueba_es.pdf', 0.8)\n",
    "resume = resume_pdfs('BOE-A-2010-4132.pdf', 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00b77e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import textract\n",
    "# PDF_read = textract.process('BOE-A-2010-4132.pdf', method='pdfminer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd380594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pdfminer.high_level import extract_text\n",
    "# PDF_read = extract_text('prueba_es.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3182b60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/34837707/how-to-extract-text-from-a-pdf-file\n",
    "# Para instalar java runtime en anaconda: https://anaconda.org/cyclus/java-jre\n",
    "from tika import parser\n",
    "\n",
    "raw = parser.from_file('prueba_es.pdf')\n",
    "print(raw['content'])\n",
    "\n",
    "texto_2 = \" \".join(raw['content'].split())\n",
    "texto_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b285d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ddedc7d",
   "metadata": {},
   "source": [
    "# Pruebas para otro proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9542ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'tes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd6d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/how-to-convert-json-into-a-pandas-dataframe-100b2ae1e0d8\n",
    "# https://stackoverflow.com/questions/56955320/file-contain-u00c2-u00a0-convert-to-characters\n",
    "\n",
    "import json\n",
    "# load data using Python JSON module\n",
    "with open('jsonformatter.json','r', encoding=['Latin1']) as f:\n",
    "    data = json.loads(f.read())\n",
    "# Flatten data\n",
    "data_json = pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b451f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/56955320/file-contain-u00c2-u00a0-convert-to-characters\n",
    "# https://jsonformatter.org/json-parser\n",
    "with open(\"jsonformatter.json\", encoding='utf8') as f:\n",
    "    data = json.loads(f.read().encode('Latin1').decode('utf8'))\n",
    "    print(data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd0cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b5be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed08c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json_formatted_1 = data_json.to_json(orient=\"split\")\n",
    "data_json_formatted_2 = data_json.to_json(orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce89abf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data_json_formatted.json', 'w', encoding='utf-8') as f:\n",
    "#     json.dump(data_json_formatted, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('data_json_formatted_split.json', 'w') as f:\n",
    "    f.write(data_json_formatted_1)\n",
    "0\n",
    "with open('data_json_formatted_records.json', 'w') as f:\n",
    "    f.write(data_json_formatted_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a82e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_json_formatted_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d6b381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
